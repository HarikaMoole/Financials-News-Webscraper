# -*- coding: utf-8 -*-
"""Scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zy7KFKq2cotSDWA30ZUSlVMXpLbboH_c
"""

# Importing necessary libraries
import requests as req
from bs4 import BeautifulSoup as BS
import pandas as pd
import logging
import boto3
from botocore.exceptions import NoCredentialsError

# Initialize logging
logging.basicConfig(level=logging.INFO)

# Initialize a boto3 client for S3
s3_client = boto3.client('s3')

def upload_to_s3(file_name, bucket, object_name=None):
    """Upload a file to an S3 bucket.

    :param file_name: File to upload
    :param bucket: Bucket to upload to
    :param object_name: S3 object name. If not specified, file_name is used
    :return: True if file was uploaded, else False
    """
    if object_name is None:
        object_name = file_name

    try:
        s3_client.upload_file(file_name, bucket, object_name)
        return True
    except NoCredentialsError:
        logging.error("Credentials not available")
        return False
    except Exception as e:
        logging.error(f"Error uploading file to S3: {e}")
        return False

def scrape_headlines():
    """Scrape headlines from websites and return them as a DataFrame."""
    headlines = pd.DataFrame(columns=["Headline"])
    urls = ["https://www.cnbc.com/economy/", "https://www.ft.com/us", "https://www.marketwatch.com/latest-news"]

    for url in urls:
        try:
            response = req.get(url)
            response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code

            soup = BS(response.content, "html.parser")
            for link in soup.find_all('a'):
                if isinstance(link.string, str) and len(link.string) > 50:
                    headlines = headlines.append({"Headline": link.string.strip()}, ignore_index=True)

        except req.exceptions.RequestException as e:
            logging.error(f"Error scraping {url}: {e}")

    return headlines

def lambda_handler(event, context):
    try:
        # Scrape headlines
        headlines = scrape_headlines()

        if headlines.empty:
            logging.error("No headlines scraped.")
            return {
                'statusCode': 500,
                'body': 'No headlines scraped.'
            }

        # File path in Lambda temporary storage
        file_name = "/tmp/headlines.csv"
        
        # Save the headlines to a CSV file in /tmp
        headlines.to_csv(file_name, index=False)

        # Name of your S3 bucket (consider retrieving this from environment variables or Lambda configuration)
        bucket_name = "automation"

        # Upload the file
        if upload_to_s3(file_name, bucket_name):
            logging.info("File uploaded successfully to S3.")
            return {
                'statusCode': 200,
                'body': 'Web scraping and S3 upload completed successfully!'
            }
        else:
            logging.error("File upload to S3 failed.")
            return {
                'statusCode': 500,
                'body': 'File upload to S3 failed.'
            }

    except Exception as e:
        logging.error(f"Error in lambda_handler: {e}")
        return {
            'statusCode': 500,
            'body': f'Error occurred: {e}'
        }

